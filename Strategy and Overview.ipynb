{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to use AWS on the 244GB instance. That should be more than enough for this 11GB dataset. I will copy the information between git repositories to make it easy.\n",
    "\n",
    "I will also use this as a chance for to test out the Bokeh and GGPLOT datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Combine all the tsv files into a single Pandas Dataframe in AWS. \n",
    "2. Make a sample of 10% of the data for a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HeadMap Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Bokeh](http://bokeh.pydata.org/en/latest/docs/gallery/cat_heatmap_chart.html)\n",
    "2. Pandas\n",
    "3. Seaborn\n",
    "4. Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the files locally from EC2 with Tableau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options\n",
    "\n",
    "1. [postgres](http://databoss.starschema.net/tableau-live-data-connection-to-csv-over-http-rest/). Might be easy with [ODO](http://odo.readthedocs.org/en/latest/perf.html) actually. \n",
    "2. SparkSQL\n",
    "3. redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Winning Model in Aquired Value Shoppers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature engineering: used Graph-Chi and social networks to develop more features\n",
    "- The dataset for Models 1 and 2 contain almost 200 features\n",
    "    1. GraphCHi features (Severla hundred gigabytes of free spcae)\n",
    "    2. Aggregations\n",
    "    3. Main Dataset PCA and matrix factorization\n",
    "    4. SNA features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregations are done in ~/Code/zach 0. Requires 256GB ram to run. Converts chain, category, id, dept, and product to factors to save ram. \n",
    "\n",
    "Main aggregation starts at line 355 where they count transactions and spend at chain, cateory, company, and brand level. They also use various two-level combinations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a Proper Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This](https://www.kaggle.com/c/avito-context-ad-clicks/forums/t/15367/proper-validation-set) post points out that there is a time trend in the ads. \n",
    "\n",
    "1. Take the minium date that occurs in the test set\n",
    "2. Take the LASt sessions for each user that happens after that date. (or sample it)/ The last session is the one with the highest date for each user\n",
    "\n",
    "//note might want to consider finding out what the time trend is here. \n",
    "\n",
    "//note also might want to consider just running the model on dates that occur after the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting Notes from the forum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my implementation, I can selectively disable features. With the full set enabled, I get results above. When I selectively disable \"user\" features (whilst keeping ad features and averages enabled), I then get results where the LB results get pulled down to validation score level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing is going to be scored in the log-loss here. $$\\textrm{LogLoss} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\\right],$$\n",
    "\n",
    "Predictions are bounded away from the extremes by a small value $(1.0 * 10 ^ {-15}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding to use for Russian Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try  encoding='cp1251'\n",
    "\n",
    ">df=pd.read_csv('~/AdsInfo.tsv',delimiter='\\t',encoding='utf-8',nrows=100)\n",
    "\n",
    ">but the column of 'Params' was encoded including the { } that showed this column is a dictionary.\n",
    "\n",
    ">u\"{817:'\\u041a\\u0443\\u0437\\u043e\\u0432', 5:'\\u0417\\u0430\\u043f\\u0447\\u0430\\u0441\\u0442\\u0438', 598:'\\u0414\\u043b\\u044f \\u0430\\u0432\\u0442\\u043e\\u043c\\u043e\\u0431\\u0438\\u043b\\u0435\\u0439'}\"\n",
    "\n",
    ">I wonder if there is a way to extract features (eg, 817, 598,...) from these encoded strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
